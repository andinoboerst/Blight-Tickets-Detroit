{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Compliance in Property Maintenance Files\n",
    "\n",
    "This project was completed as part of the \"[Applied Machine Learning in Python](https://www.coursera.org/learn/python-machine-learning)\" course by the University of Michigan on Coursera.\n",
    "\n",
    "The dataset uploaded with this file was provided during this course and is based on a Kaggle competition (which includes the datasets) found here: [https://www.kaggle.com/competitions/detroit-blight-ticket-compliance/data](https://www.kaggle.com/competitions/detroit-blight-ticket-compliance/data).\n",
    "\n",
    "This dataset is about blight tickets issued in the Detroit area, showing the compliance to paying these tickets. The goal is to develop a classification model that returns the probability for future cases to be compliant. The cases to be returned are provided in the 'test.csv' file and the training data is provided in the 'train.csv' file. Additionally there are two more .csv files containing information about addresses and their latitude and longitudonal coordinates.\n",
    "\n",
    "This notebook goes through the whole process of:\n",
    "* Processing and cleaning the data\n",
    "* Preparing the data for classification\n",
    "* Training the classification model with GridSearch to find optimal hyperparameters\n",
    "* Predicting the test data\n",
    "\n",
    "The class is set to either (0 for not compliant), (1 for compliant) or (NaN for not responsible) in the \"compliance\" feature of the train data set.\n",
    "\n",
    "All of the features in the train and test datasets are shown below.\n",
    "\n",
    "<br>\n",
    "\n",
    "    readonly/train.csv - the training set (all tickets issued 2004-2011)\n",
    "    readonly/test.csv - the test set (all tickets issued 2012-2016)\n",
    "    readonly/addresses.csv & readonly/latlons.csv - mapping from ticket id to addresses, and from addresses to lat/lon coordinates. \n",
    "     Note: misspelled addresses may be incorrectly geolocated.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Data fields**\n",
    "\n",
    "train.csv & test.csv\n",
    "\n",
    "    ticket_id - unique identifier for tickets\n",
    "    agency_name - Agency that issued the ticket\n",
    "    inspector_name - Name of inspector that issued the ticket\n",
    "    violator_name - Name of the person/organization that the ticket was issued to\n",
    "    violation_street_number, violation_street_name, violation_zip_code - Address where the violation occurred\n",
    "    mailing_address_str_number, mailing_address_str_name, city, state, zip_code, non_us_str_code, country - Mailing address of the violator\n",
    "    ticket_issued_date - Date and time the ticket was issued\n",
    "    hearing_date - Date and time the violator's hearing was scheduled\n",
    "    violation_code, violation_description - Type of violation\n",
    "    disposition - Judgment and judgement type\n",
    "    fine_amount - Violation fine amount, excluding fees\n",
    "    admin_fee - $20 fee assigned to responsible judgments\n",
    "state_fee - $10 fee assigned to responsible judgments\n",
    "    late_fee - 10% fee assigned to responsible judgments\n",
    "    discount_amount - discount applied, if any\n",
    "    clean_up_cost - DPW clean-up or graffiti removal cost\n",
    "    judgment_amount - Sum of all fines and fees\n",
    "    grafitti_status - Flag for graffiti violations\n",
    "    \n",
    "train.csv only\n",
    "\n",
    "    payment_amount - Amount paid, if any\n",
    "    payment_date - Date payment was made, if it was received\n",
    "    payment_status - Current payment status as of Feb 1 2017\n",
    "    balance_due - Fines and fees still owed\n",
    "    collection_status - Flag for payments in collections\n",
    "    compliance [target variable for prediction] \n",
    "     Null = Not responsible\n",
    "     0 = Responsible, non-compliant\n",
    "     1 = Responsible, compliant\n",
    "    compliance_detail - More information on why each ticket was marked compliant or non-compliant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function used to show if the data is complete or if any values are still missing for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_update(df):\n",
    "    total_length = len(df)\n",
    "    for column in df.columns:\n",
    "        print(f'{column}: {df[column].isnull().sum()}/{total_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('readonly/train.csv', encoding = \"ISO-8859-1\", low_memory=False) # The training set\n",
    "df_test = pd.read_csv('readonly/test.csv') # The testing set (does not contain information on the class)\n",
    "df_address = pd.read_csv('readonly/addresses.csv')\n",
    "df_latlon = pd.read_csv('readonly/latlons.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will look at the data to see how many datapoints are included in each df and what the dtype of each column is, as well as what columns are included in each df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250306 entries, 0 to 250305\n",
      "Data columns (total 34 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   ticket_id                   250306 non-null  int64  \n",
      " 1   agency_name                 250306 non-null  object \n",
      " 2   inspector_name              250306 non-null  object \n",
      " 3   violator_name               250272 non-null  object \n",
      " 4   violation_street_number     250306 non-null  float64\n",
      " 5   violation_street_name       250306 non-null  object \n",
      " 6   violation_zip_code          0 non-null       float64\n",
      " 7   mailing_address_str_number  246704 non-null  float64\n",
      " 8   mailing_address_str_name    250302 non-null  object \n",
      " 9   city                        250306 non-null  object \n",
      " 10  state                       250213 non-null  object \n",
      " 11  zip_code                    250305 non-null  object \n",
      " 12  non_us_str_code             3 non-null       object \n",
      " 13  country                     250306 non-null  object \n",
      " 14  ticket_issued_date          250306 non-null  object \n",
      " 15  hearing_date                237815 non-null  object \n",
      " 16  violation_code              250306 non-null  object \n",
      " 17  violation_description       250306 non-null  object \n",
      " 18  disposition                 250306 non-null  object \n",
      " 19  fine_amount                 250305 non-null  float64\n",
      " 20  admin_fee                   250306 non-null  float64\n",
      " 21  state_fee                   250306 non-null  float64\n",
      " 22  late_fee                    250306 non-null  float64\n",
      " 23  discount_amount             250306 non-null  float64\n",
      " 24  clean_up_cost               250306 non-null  float64\n",
      " 25  judgment_amount             250306 non-null  float64\n",
      " 26  payment_amount              250306 non-null  float64\n",
      " 27  balance_due                 250306 non-null  float64\n",
      " 28  payment_date                41113 non-null   object \n",
      " 29  payment_status              250306 non-null  object \n",
      " 30  collection_status           36897 non-null   object \n",
      " 31  grafitti_status             1 non-null       object \n",
      " 32  compliance_detail           250306 non-null  object \n",
      " 33  compliance                  159880 non-null  float64\n",
      "dtypes: float64(13), int64(1), object(20)\n",
      "memory usage: 64.9+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61001 entries, 0 to 61000\n",
      "Data columns (total 27 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   ticket_id                   61001 non-null  int64  \n",
      " 1   agency_name                 61001 non-null  object \n",
      " 2   inspector_name              61001 non-null  object \n",
      " 3   violator_name               60973 non-null  object \n",
      " 4   violation_street_number     61001 non-null  float64\n",
      " 5   violation_street_name       61001 non-null  object \n",
      " 6   violation_zip_code          24024 non-null  object \n",
      " 7   mailing_address_str_number  59987 non-null  object \n",
      " 8   mailing_address_str_name    60998 non-null  object \n",
      " 9   city                        61000 non-null  object \n",
      " 10  state                       60670 non-null  object \n",
      " 11  zip_code                    60998 non-null  object \n",
      " 12  non_us_str_code             0 non-null      float64\n",
      " 13  country                     61001 non-null  object \n",
      " 14  ticket_issued_date          61001 non-null  object \n",
      " 15  hearing_date                58804 non-null  object \n",
      " 16  violation_code              61001 non-null  object \n",
      " 17  violation_description       61001 non-null  object \n",
      " 18  disposition                 61001 non-null  object \n",
      " 19  fine_amount                 61001 non-null  float64\n",
      " 20  admin_fee                   61001 non-null  float64\n",
      " 21  state_fee                   61001 non-null  float64\n",
      " 22  late_fee                    61001 non-null  float64\n",
      " 23  discount_amount             61001 non-null  float64\n",
      " 24  clean_up_cost               61001 non-null  float64\n",
      " 25  judgment_amount             61001 non-null  float64\n",
      " 26  grafitti_status             2221 non-null   object \n",
      "dtypes: float64(9), int64(1), object(17)\n",
      "memory usage: 12.6+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 311307 entries, 0 to 311306\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   ticket_id  311307 non-null  int64 \n",
      " 1   address    311307 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 4.8+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 121769 entries, 0 to 121768\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   address  121769 non-null  object \n",
      " 1   lat      121762 non-null  float64\n",
      " 2   lon      121762 non-null  float64\n",
      "dtypes: float64(2), object(1)\n",
      "memory usage: 2.8+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_train.info())\n",
    "print(df_test.info())\n",
    "print(df_address.info())\n",
    "print(df_latlon.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can see that the train dataset contains 34 columns and 250305 entries. The test dataset only contains 27 columns, which means I need to check which columns will not be available when classifying the test set to not train the model on non-existing data.\n",
    "The address dataset is complete and the longitute and latitude dataset is only missing 7 values.\n",
    "\n",
    "Additionally I noticed that there were 3 entries for 'non_us_str_code', which means I will check if there are any entries in the test set outside the USA or if these entries can be discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "0\n",
      "        ticket_id                                     agency_name  \\\n",
      "160652     189866                      Department of Public Works   \n",
      "177864     209821                      Department of Public Works   \n",
      "178520     209675                      Department of Public Works   \n",
      "211755     245108  Buildings, Safety Engineering & Env Department   \n",
      "216567     250642                      Department of Public Works   \n",
      "216568     250643                      Department of Public Works   \n",
      "216927     250552                      Department of Public Works   \n",
      "217529     251516                      Department of Public Works   \n",
      "222230     256184                      Department of Public Works   \n",
      "226259     259979                      Department of Public Works   \n",
      "226609     259990                      Department of Public Works   \n",
      "226610     259991                      Department of Public Works   \n",
      "236075     270253                      Department of Public Works   \n",
      "\n",
      "            inspector_name                violator_name  \\\n",
      "160652          Gray, Paul              SCHNITTER, JEFF   \n",
      "177864  Havard, Jacqueline             ANDERTON, ANDREW   \n",
      "178520      Baker, Kenneth             ANDERTON, ANDREW   \n",
      "211755    Steele, Jonathan                   MAI, QUACH   \n",
      "216567     McCants, Angela               BOTROS, BISHOP   \n",
      "216568     McCants, Angela               BOTROS, BISHOP   \n",
      "216927          May, Tanya          PARMINDER SOROYA, .   \n",
      "217529          May, Tanya          PARMINDER SOROYA, .   \n",
      "222230      Davis, Darlene             DESMARAIS, ROGER   \n",
      "226259     McCants, Angela          JAHANGIRI, MOHAMMAD   \n",
      "226609      Davis, Darlene        LEGEND REAL ESTATE, .   \n",
      "226610      Davis, Darlene        LEGEND REAL ESTATE, .   \n",
      "236075     McCants, Angela  INVESTMENT, GELLER PROPERTY   \n",
      "\n",
      "        violation_street_number violation_street_name  violation_zip_code  \\\n",
      "160652                  19715.0           ASBURY PARK                 NaN   \n",
      "177864                   8897.0             EVERGREEN                 NaN   \n",
      "178520                   2126.0                PIERCE                 NaN   \n",
      "211755                   8530.0               HUBBELL                 NaN   \n",
      "216567                   5575.0              MARYLAND                 NaN   \n",
      "216568                   5575.0              MARYLAND                 NaN   \n",
      "216927                   5239.0               IVANHOE                 NaN   \n",
      "217529                   9071.0                BRYDEN                 NaN   \n",
      "222230                  19266.0              SYRACUSE                 NaN   \n",
      "226259                   5581.0               WAYBURN                 NaN   \n",
      "226609                   9911.0          BEACONSFIELD                 NaN   \n",
      "226610                   9911.0          BEACONSFIELD                 NaN   \n",
      "236075                   7700.0                HARPER                 NaN   \n",
      "\n",
      "        mailing_address_str_number      mailing_address_str_name  \\\n",
      "160652                         2.0                   ANTIQUE DR.   \n",
      "177864                       238.0            ROBINA TOWN CENTER   \n",
      "178520                       238.0            ROBINA TOWN CENTER   \n",
      "211755                     25359.0                   16TH AVENUE   \n",
      "216567                         1.0                        TAIMOR   \n",
      "216568                         1.0                     TAIMOR ST   \n",
      "216927                        47.0                 ELMONT  DRIVE   \n",
      "217529                        47.0                    ELMONT DR.   \n",
      "222230                         NaN                   P O BOX 717   \n",
      "226259                         NaN         ROSA-REINGLASS-STEIG2   \n",
      "226609                        65.0  WYNFORD HTS. CRES, STE. 1905   \n",
      "226610                        65.0  WYNFORD HTS. CRES, STE. 1905   \n",
      "236075                       111.0                     LIVERPOOL   \n",
      "\n",
      "                                           city  ... clean_up_cost  \\\n",
      "160652                            RICHMOND HILL  ...           0.0   \n",
      "177864                               QUEENSLAND  ...           0.0   \n",
      "178520                                   ROBINA  ...           0.0   \n",
      "211755                               ALDERGROVE  ...           0.0   \n",
      "216567  ST. FATIMA SQ. HELIOPOLIS, CAIRO, EGYPT  ...           0.0   \n",
      "216568  ST. FATIMA SQ. HELIOPOLIS, CAIRO, EGYPT  ...           0.0   \n",
      "216927                 CALGARY, ALBERTA, CANADA  ...           0.0   \n",
      "217529                 CALGARY, ALBERTA, CANADA  ...           0.0   \n",
      "222230                                SUN RIDGE  ...           0.0   \n",
      "226259                                   BERLIN  ...           0.0   \n",
      "226609                                  TORONTO  ...           0.0   \n",
      "226610                                  TORONTO  ...           0.0   \n",
      "236075                      ST ROSEBAY AUSTRALI  ...           0.0   \n",
      "\n",
      "       judgment_amount payment_amount balance_due         payment_date  \\\n",
      "160652            85.0            0.0        85.0                  NaN   \n",
      "177864           130.0          130.0         0.0  2009-06-01 00:00:00   \n",
      "178520             0.0            0.0         0.0                  NaN   \n",
      "211755           280.0          255.0        25.0  2010-04-21 00:00:00   \n",
      "216567            85.0          165.0       -80.0  2010-09-10 00:00:00   \n",
      "216568            85.0           85.0         0.0  2010-06-29 00:00:00   \n",
      "216927            85.0            0.0        85.0                  NaN   \n",
      "217529             0.0            0.0         0.0                  NaN   \n",
      "222230            85.0            0.0        85.0                  NaN   \n",
      "226259            85.0            0.0        85.0                  NaN   \n",
      "226609            85.0            0.0        85.0                  NaN   \n",
      "226610           250.0            0.0       250.0                  NaN   \n",
      "236075           140.0            0.0       140.0                  NaN   \n",
      "\n",
      "                 payment_status collection_status grafitti_status  \\\n",
      "160652       NO PAYMENT APPLIED               NaN             NaN   \n",
      "177864             PAID IN FULL               NaN             NaN   \n",
      "178520       NO PAYMENT APPLIED               NaN             NaN   \n",
      "211755  PARTIAL PAYMENT APPLIED               NaN             NaN   \n",
      "216567             PAID IN FULL               NaN             NaN   \n",
      "216568             PAID IN FULL               NaN             NaN   \n",
      "216927       NO PAYMENT APPLIED               NaN             NaN   \n",
      "217529       NO PAYMENT APPLIED               NaN             NaN   \n",
      "222230       NO PAYMENT APPLIED               NaN             NaN   \n",
      "226259       NO PAYMENT APPLIED               NaN             NaN   \n",
      "226609       NO PAYMENT APPLIED               NaN             NaN   \n",
      "226610       NO PAYMENT APPLIED               NaN             NaN   \n",
      "236075       NO PAYMENT APPLIED               NaN             NaN   \n",
      "\n",
      "                                      compliance_detail  compliance  \n",
      "160652                      non-compliant by no payment         0.0  \n",
      "177864                     compliant by on-time payment         1.0  \n",
      "178520                   not responsible by disposition         NaN  \n",
      "211755                      non-compliant by no payment         0.0  \n",
      "216567  non-compliant by late payment more than 1 month         0.0  \n",
      "216568         compliant by late payment within 1 month         1.0  \n",
      "216927                      non-compliant by no payment         0.0  \n",
      "217529                   not responsible by disposition         NaN  \n",
      "222230                      non-compliant by no payment         0.0  \n",
      "226259                      non-compliant by no payment         0.0  \n",
      "226609                      non-compliant by no payment         0.0  \n",
      "226610                      non-compliant by no payment         0.0  \n",
      "236075                      non-compliant by no payment         0.0  \n",
      "\n",
      "[13 rows x 34 columns]\n",
      "Empty DataFrame\n",
      "Columns: [city, country]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train[df_train['country']!='USA']))\n",
    "print(len(df_test[df_test['country']!='USA']))\n",
    "print(df_train[df_train['country']!='USA'])\n",
    "print(df_test.loc[df_test['country'].isna(),['city','country']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it can be seen that there are 13 entries in the train set which are outside the USA, but none in the test set. This means these testcases are most likely special cases and I will not consider them further in the test set. \n",
    "This means I will drop all the cases with countries outside the USA as well as the test cases which show the person to not be responsible (i.e. compliance=NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "df_train = df_train[df_train['country']=='USA'] # Drop all entries that are not from the US\n",
    "df_train = df_train.dropna(subset=['compliance']) # Drop all NaNs in compliance\n",
    "df_train.drop(['country'], axis=1, inplace=True) # Can be dropped, since is all USA now\n",
    "df_test.drop(['country'], axis=1, inplace=True) # Can be dropped, since is all USA now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now inspect the columns which are not included in the test set to see if they contain any information that could be used to fill any of the NaN values in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   payment_amount  balance_due         payment_date      payment_status  \\\n",
      "0             0.0        305.0                  NaN  NO PAYMENT APPLIED   \n",
      "1           780.0         75.0  2005-06-02 00:00:00        PAID IN FULL   \n",
      "5             0.0        305.0                  NaN  NO PAYMENT APPLIED   \n",
      "6             0.0        855.0                  NaN  NO PAYMENT APPLIED   \n",
      "7             0.0        140.0                  NaN  NO PAYMENT APPLIED   \n",
      "\n",
      "  collection_status                         compliance_detail  \n",
      "0               NaN               non-compliant by no payment  \n",
      "1               NaN  compliant by late payment within 1 month  \n",
      "5               NaN               non-compliant by no payment  \n",
      "6               NaN               non-compliant by no payment  \n",
      "7               NaN               non-compliant by no payment  \n"
     ]
    }
   ],
   "source": [
    "print(df_train[[column for column in df_train.columns if column not in np.append(df_test.columns, 'compliance')]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the only columns that are not contained in the test set but indeed in the training set are related to the payment/compliance of the ticket. This means all of these columns can be dropped, as they do not contain any useful information for filling NaN values and are not available for prediction later on.\n",
    "Additionally the address and longitude and latitude data sets are merged with the trianing and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns not available in test\n",
    "df_train.drop([column for column in df_train.columns if column not in np.append(df_test.columns, 'compliance')], axis=1, inplace=True)\n",
    "\n",
    "# merge addresse information\n",
    "df_ad_ll = pd.merge(df_address, df_latlon, on='address', how='left')\n",
    "df_train = pd.merge(df_train, df_ad_ll, how='left', on='ticket_id')\n",
    "df_test = pd.merge(df_test, df_ad_ll, how='left', on='ticket_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it would be helpful to look again at how many NaN values are in each of the two train and test sets. For this I will call the give_update() function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id: 0/159869\n",
      "agency_name: 0/159869\n",
      "inspector_name: 0/159869\n",
      "violator_name: 26/159869\n",
      "violation_street_number: 0/159869\n",
      "violation_street_name: 0/159869\n",
      "violation_zip_code: 159869/159869\n",
      "mailing_address_str_number: 2556/159869\n",
      "mailing_address_str_name: 3/159869\n",
      "city: 0/159869\n",
      "state: 80/159869\n",
      "zip_code: 1/159869\n",
      "non_us_str_code: 159869/159869\n",
      "ticket_issued_date: 0/159869\n",
      "hearing_date: 227/159869\n",
      "violation_code: 0/159869\n",
      "violation_description: 0/159869\n",
      "disposition: 0/159869\n",
      "fine_amount: 0/159869\n",
      "admin_fee: 0/159869\n",
      "state_fee: 0/159869\n",
      "late_fee: 0/159869\n",
      "discount_amount: 0/159869\n",
      "clean_up_cost: 0/159869\n",
      "judgment_amount: 0/159869\n",
      "grafitti_status: 159869/159869\n",
      "compliance: 0/159869\n",
      "address: 0/159869\n",
      "lat: 2/159869\n",
      "lon: 2/159869\n",
      "\n",
      "\n",
      "ticket_id: 0/61001\n",
      "agency_name: 0/61001\n",
      "inspector_name: 0/61001\n",
      "violator_name: 28/61001\n",
      "violation_street_number: 0/61001\n",
      "violation_street_name: 0/61001\n",
      "violation_zip_code: 36977/61001\n",
      "mailing_address_str_number: 1014/61001\n",
      "mailing_address_str_name: 3/61001\n",
      "city: 1/61001\n",
      "state: 331/61001\n",
      "zip_code: 3/61001\n",
      "non_us_str_code: 61001/61001\n",
      "ticket_issued_date: 0/61001\n",
      "hearing_date: 2197/61001\n",
      "violation_code: 0/61001\n",
      "violation_description: 0/61001\n",
      "disposition: 0/61001\n",
      "fine_amount: 0/61001\n",
      "admin_fee: 0/61001\n",
      "state_fee: 0/61001\n",
      "late_fee: 0/61001\n",
      "discount_amount: 0/61001\n",
      "clean_up_cost: 0/61001\n",
      "judgment_amount: 0/61001\n",
      "grafitti_status: 58780/61001\n",
      "address: 0/61001\n",
      "lat: 5/61001\n",
      "lon: 5/61001\n"
     ]
    }
   ],
   "source": [
    "give_update(df_train)\n",
    "print('\\n')\n",
    "give_update(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that the 'graffiti_status' and 'violation_zip_code' contains only NaN values in the train set, which means this column can be dropped.\n",
    "\n",
    "The addresses will be inspected further to see if information might be included as a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             address   zip_code  violation_street_number  \\\n",
      "0             2900 tyler, Detroit MI      60606                   2900.0   \n",
      "1           4311 central, Detroit MI      48208                   4311.0   \n",
      "2        6478 northfield, Detroit MI  908041512                   6478.0   \n",
      "3         8027 brentwood, Detroit MI      48038                   8027.0   \n",
      "4        8228 mt elliott, Detroit MI      48211                   8228.0   \n",
      "...                              ...        ...                      ...   \n",
      "159864   20009 northlawn, Detroit MI      48235                  20009.0   \n",
      "159865       15725 steel, Detroit MI      48227                  15725.0   \n",
      "159866      7152 chicago, Detroit MI      48204                   7152.0   \n",
      "159867  17403 mt elliott, Detroit MI      48212                  17403.0   \n",
      "159868      15634 novara, Detroit MI      95926                  15634.0   \n",
      "\n",
      "       violation_street_name  violation_zip_code  mailing_address_str_number  \\\n",
      "0                      TYLER                 NaN                         3.0   \n",
      "1                    CENTRAL                 NaN                      2959.0   \n",
      "2                 NORTHFIELD                 NaN                      2755.0   \n",
      "3                  BRENTWOOD                 NaN                       476.0   \n",
      "4                 MT ELLIOTT                 NaN                      8228.0   \n",
      "...                      ...                 ...                         ...   \n",
      "159864             NORTHLAWN                 NaN                     18715.0   \n",
      "159865                 STEEL                 NaN                         NaN   \n",
      "159866               CHICAGO                 NaN                      7152.0   \n",
      "159867            MT ELLIOTT                 NaN                      1743.0   \n",
      "159868                NOVARA                 NaN                        72.0   \n",
      "\n",
      "       mailing_address_str_name  \n",
      "0                     S. WICKER  \n",
      "1            Martin Luther King  \n",
      "2                       E. 17TH  \n",
      "3                      Garfield  \n",
      "4                   Mt. Elliott  \n",
      "...                         ...  \n",
      "159864                  MARLOWE  \n",
      "159865           P.O. BOX 27125  \n",
      "159866               W. CHICAGO  \n",
      "159867               MT ELLIOTT  \n",
      "159868                 MANGROVE  \n",
      "\n",
      "[159869 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_train[['address', 'zip_code','violation_street_number','violation_street_name','violation_zip_code','mailing_address_str_number','mailing_address_str_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the 'addresse' column is the same as the 'violator_...' columns. This means the violator columns will be dropped. Additionally the columns relating to the mailing addresse will be dropped as well, apart from the zip-code, as otherwise too much information about this is contained in the data set and the zip code is already in numerical format. It needs to be verified however that the zip code only has length of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I try to clean up the zip-code column with the city names. This works well for the trianing set. However with teh test set I notice some problems, as the three entries with missing zip-code are in fact not in the USA after all. These three entries will be classified as non-compliant and will not be predicted with the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_train[df_train['zip_code'].isnull()].iterrows():\n",
    "    estimated = df_train.loc[df_train['city']==row['city'],'zip_code'].dropna()\n",
    "    if len(estimated)!=0:\n",
    "        df_train.loc[index,'zip_code'] = estimated.iloc[0]\n",
    "\n",
    "list_non_compliant_test = []\n",
    "for index, row in df_test[df_test['zip_code'].isnull()].iterrows():\n",
    "    list_non_compliant_test.append(row['ticket_id']) # these will all get a score of 0 in the final prediction\n",
    "\n",
    "df_test = df_test[~df_test['ticket_id'].isin(list_non_compliant_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I clean up the lat and lon columns using the violation street name, before dropping all of the unnecessary columns.\n",
    "If I cannot find a corresponding street name, I will take the mean of the whole dataset to fill the NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill lat and lon with mean of same city \n",
    "\n",
    "for index, row in df_train[df_train['lat'].isnull()].iterrows():\n",
    "    estimated = df_train.loc[df_train['violation_street_name'].str.lower()==row['violation_street_name'].lower(),['lat','lon']].dropna()\n",
    "    if len(estimated)==0:\n",
    "        df_train.loc[index,'lat'] = df_train['lat'].dropna().mean()\n",
    "        df_train.loc[index,'lon'] = df_train['lon'].dropna().mean()\n",
    "    else:\n",
    "        df_train.loc[index,'lat'] = estimated['lat'].mean()\n",
    "        df_train.loc[index,'lon'] = estimated['lon'].mean()\n",
    "\n",
    "for index, row in df_test[df_test['lat'].isnull()].iterrows():\n",
    "    estimated = df_test.loc[df_test['violation_street_name'].str.lower()==row['violation_street_name'].lower(),['lat','lon']].dropna()\n",
    "    if len(estimated)==0:\n",
    "        df_test.loc[index,'lat'] = df_train['lat'].dropna().mean()\n",
    "        df_test.loc[index,'lon'] = df_train['lon'].dropna().mean()\n",
    "    else:\n",
    "        df_test.loc[index,'lat'] = estimated['lat'].mean()\n",
    "        df_test.loc[index,'lon'] = estimated['lon'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for any zip codes longer than 5 digits. If cannot find substitute, drop them from train set or add them to non compliant list in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_train.loc[df_train['zip_code'].str.len() > 5].iterrows():\n",
    "    estimated = df_train.loc[df_train['city']==row['city'],'zip_code'].dropna()\n",
    "    if len(estimated)!=0:\n",
    "        df_train.loc[index,'zip_code'] = estimated.iloc[0]\n",
    "\n",
    "for index, row in df_train.loc[df_train['zip_code'].str.len() > 5].iterrows():\n",
    "    estimated = df_train.loc[df_train['mailing_address_str_name']==row['mailing_address_str_name'],'zip_code'].dropna()\n",
    "    if len(estimated)!=0:\n",
    "        df_train.loc[index,'zip_code'] = estimated.iloc[0]\n",
    "\n",
    "df_train = df_train.loc[df_train['zip_code'].str.len() <= 5]\n",
    "df_train['zip_code'] = pd.to_numeric(df_train['zip_code'], errors='coerce')\n",
    "df_train = df_train[~df_train['zip_code'].isna()]\n",
    "\n",
    "for index, row in df_test.loc[df_test['zip_code'].str.len() > 5].iterrows():\n",
    "    estimated = df_train.loc[df_train['city']==row['city'],'zip_code'].dropna()\n",
    "    if len(estimated)!=0:\n",
    "        df_test.loc[index,'zip_code'] = estimated.iloc[0]\n",
    "\n",
    "for index, row in df_test.loc[df_test['zip_code'].str.len() > 5].iterrows():\n",
    "    estimated = df_train.loc[df_train['mailing_address_str_name']==row['mailing_address_str_name'],'zip_code'].dropna()\n",
    "    if len(estimated)!=0:\n",
    "        df_test.loc[index,'zip_code'] = estimated.iloc[0]\n",
    "    else:\n",
    "        list_non_compliant_test.append(row['ticket_id'])\n",
    "\n",
    "df_test['zip_code'] = pd.to_numeric(df_test['zip_code'], errors='coerce')\n",
    "list_non_compliant_test.extend(df_test.loc[df_test['zip_code'].isna(),'ticket_id'].to_list())\n",
    "df_test = df_test[~df_test['ticket_id'].isin(list_non_compliant_test)]\n",
    "df_test = df_test[~df_test['zip_code'].isna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['address',\n",
    "            'city',\n",
    "            'violation_street_number',\n",
    "            'violation_street_name',\n",
    "            'violation_zip_code',\n",
    "            'mailing_address_str_number',\n",
    "            'mailing_address_str_name',\n",
    "            'violator_name',\n",
    "            'non_us_str_code',\n",
    "            'violation_description',\n",
    "            'grafitti_status',\n",
    "            'state']\n",
    "df_train.drop(to_drop, axis=1, inplace=True)\n",
    "df_test.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticket_id: 0/159833\n",
      "agency_name: 0/159833\n",
      "inspector_name: 0/159833\n",
      "zip_code: 0/159833\n",
      "ticket_issued_date: 0/159833\n",
      "hearing_date: 227/159833\n",
      "violation_code: 0/159833\n",
      "disposition: 0/159833\n",
      "fine_amount: 0/159833\n",
      "admin_fee: 0/159833\n",
      "state_fee: 0/159833\n",
      "late_fee: 0/159833\n",
      "discount_amount: 0/159833\n",
      "clean_up_cost: 0/159833\n",
      "judgment_amount: 0/159833\n",
      "compliance: 0/159833\n",
      "lat: 0/159833\n",
      "lon: 0/159833\n",
      "\n",
      "\n",
      "ticket_id: 0/60873\n",
      "agency_name: 0/60873\n",
      "inspector_name: 0/60873\n",
      "zip_code: 0/60873\n",
      "ticket_issued_date: 0/60873\n",
      "hearing_date: 2197/60873\n",
      "violation_code: 0/60873\n",
      "disposition: 0/60873\n",
      "fine_amount: 0/60873\n",
      "admin_fee: 0/60873\n",
      "state_fee: 0/60873\n",
      "late_fee: 0/60873\n",
      "discount_amount: 0/60873\n",
      "clean_up_cost: 0/60873\n",
      "judgment_amount: 0/60873\n",
      "lat: 0/60873\n",
      "lon: 0/60873\n",
      "ticket_id               int64\n",
      "agency_name            object\n",
      "inspector_name         object\n",
      "zip_code                int64\n",
      "ticket_issued_date     object\n",
      "hearing_date           object\n",
      "violation_code         object\n",
      "disposition            object\n",
      "fine_amount           float64\n",
      "admin_fee             float64\n",
      "state_fee             float64\n",
      "late_fee              float64\n",
      "discount_amount       float64\n",
      "clean_up_cost         float64\n",
      "judgment_amount       float64\n",
      "compliance            float64\n",
      "lat                   float64\n",
      "lon                   float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "give_update(df_train)\n",
    "print('\\n')\n",
    "give_update(df_test)\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I dropped all of the unnecessary columns and I only have informative columns left and no duplicate information.\n",
    "The only data that is left with NaN values is the hearing date column.\n",
    "For this column I will calculate the average time it takes from the ticket being issued and the hearing.\n",
    "Additionally I will create a new column which contains exactly this time difference and I will drop the two date columns.\n",
    "\n",
    "Additionally I will convert all remaining string columns to lowercase letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert strings to lowercase letters\n",
    "to_lower = ['agency_name', 'inspector_name', 'disposition']\n",
    "for name in to_lower:\n",
    "    df_train[name] = df_train[name].str.lower()\n",
    "    df_test[name] = df_test[name].str.lower()\n",
    "    \n",
    "# Calculate the gap between hearing date and ticket issued date and convert it to int\n",
    "df_train[['ticket_issued_date','hearing_date']] = df_train[['ticket_issued_date','hearing_date']].apply(pd.to_datetime)\n",
    "df_test[['ticket_issued_date','hearing_date']] = df_test[['ticket_issued_date','hearing_date']].apply(pd.to_datetime)\n",
    "    \n",
    "int_df = pd.concat([df_train, df_test], ignore_index=True)[['ticket_issued_date','hearing_date']].dropna()\n",
    "avg_date_gap = (int_df['hearing_date']-int_df['ticket_issued_date']).mean().days\n",
    "\n",
    "df_train['date_gap'] = df_train['hearing_date']-df_train['ticket_issued_date']\n",
    "df_test['date_gap'] = df_test['hearing_date']-df_test['ticket_issued_date']\n",
    "\n",
    "df_train.loc[df_train['hearing_date'].isnull(),'date_gap'] = avg_date_gap\n",
    "df_test.loc[df_test['hearing_date'].isnull(),'date_gap'] = avg_date_gap\n",
    "\n",
    "df_train.drop(['hearing_date','ticket_issued_date'], axis=1,inplace=True)\n",
    "df_test.drop(['hearing_date','ticket_issued_date'], axis=1,inplace=True)\n",
    "\n",
    "df_train['date_gap'] = pd.to_timedelta(df_train['date_gap']).dt.days.astype('int16')\n",
    "df_test['date_gap'] = pd.to_timedelta(df_test['date_gap']).dt.days.astype('int16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the classification model to be able to process the strings I will convert these to classes instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string values to categories need to convert agency_name, city, violation_code and disposition\n",
    "#print(df_train.dtypes)\n",
    "\n",
    "list_convert = ['agency_name', 'violation_code', 'disposition', 'inspector_name']\n",
    "for col in list_convert:\n",
    "    dict_list = {x: i for i, x in enumerate(pd.concat([df_train,df_test], ignore_index=True)[col].unique())}\n",
    "    df_train[col] = [dict_list[x] for x in df_train[col]]\n",
    "    df_test[col] = [dict_list[x] for x in df_test[col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the data is ready for the classification.\n",
    "The classification model chosen here is the Gradien Boosting Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train.drop(['compliance'], axis=1), df_train['compliance'], random_state = 0)\n",
    "\n",
    "clf = GradientBoostingClassifier()\n",
    "grid_values = {'n_estimators': [10, 50], 'learning_rate': [0.1,1], 'max_depth': [7]} # Not more for computational cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 1], &#x27;max_depth&#x27;: [7],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 1], &#x27;max_depth&#x27;: [7],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 1], 'max_depth': [7],\n",
       "                         'n_estimators': [10, 50]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf = GridSearchCV(clf, param_grid = grid_values, scoring='roc_auc')\n",
    "\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set AUC:  0.8376045589947625\n",
      "Grid best parameter (max. AUC):  {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}\n",
      "Grid best score (AUC):  0.8344513902641933\n"
     ]
    }
   ],
   "source": [
    "y_decision_scores = grid_clf.decision_function(X_test)\n",
    "\n",
    "print('Test set AUC: ', roc_auc_score(y_test, y_decision_scores))\n",
    "print('Grid best parameter (max. AUC): ', grid_clf.best_params_)\n",
    "print('Grid best score (AUC): ', grid_clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 1], &#x27;max_depth&#x27;: [7],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=GradientBoostingClassifier(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 1], &#x27;max_depth&#x27;: [7],\n",
       "                         &#x27;n_estimators&#x27;: [10, 50]},\n",
       "             scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': [0.1, 1], 'max_depth': [7],\n",
       "                         'n_estimators': [10, 50]},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.fit(df_train.drop(['compliance'], axis=1), df_train['compliance'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61001\n",
      "                  0\n",
      "ticket_id          \n",
      "284932     0.075345\n",
      "285362     0.036131\n",
      "285361     0.080321\n",
      "285338     0.090308\n",
      "285346     0.095118\n",
      "...             ...\n",
      "355405     0.000000\n",
      "356718     0.000000\n",
      "360292     0.000000\n",
      "364067     0.000000\n",
      "365224     0.000000\n",
      "\n",
      "[61001 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "y_decision_scores_test = grid_clf.predict_proba(df_test)\n",
    "\n",
    "final_pred = pd.DataFrame(y_decision_scores_test[:,1],index=df_test['ticket_id'])\n",
    "for entry in list_non_compliant_test:\n",
    "    final_pred.loc[entry] = 0\n",
    "\n",
    "print(len(final_pred))\n",
    "\n",
    "print(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f43aa21a5197a0f4cf262743c7b38eb72cc69c0c4bcc6da2bc0a24be8b7d537a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
